{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from align import AlignDlib\n",
    "%matplotlib inline\n",
    "\n",
    "alignment = AlignDlib('models/landmarks.dat')\n",
    "def load_image(path):\n",
    "    img = cv2.imread(path, 1)\n",
    "    return img[...,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_embeddings = list(np.load('embedded_face.npy') )\n",
    "face_emb_labels = list(np.load('embedded_face_labels.npy'))\n",
    "voice_embeddings= list(np.load('voice_embeddings_input.npy'))\n",
    "voice_emb_labels= list(np.load('voice_embeddings_labels.npy'))\n",
    "\n",
    "voice_embedd = []\n",
    "voice_embedd_labels = []\n",
    "face_npy = []\n",
    "\n",
    "for k in [0,1,2,3,4,5,6,7,8]:\n",
    "    count = 0\n",
    "    limit = face_emb_labels.count(k)\n",
    "    for i in range(len(voice_emb_labels)):\n",
    "        if(count==limit):\n",
    "            break\n",
    "        else:    \n",
    "            if(voice_emb_labels[i]==k):\n",
    "                voice_embedd.append(voice_embeddings[i])\n",
    "                voice_embedd_labels.append(voice_emb_labels[i])\n",
    "                count+=1\n",
    "            \n",
    "directory_path = 'Dataset'\n",
    "directory_path1 = 'Dataset1'\n",
    "face_npy = []\n",
    "for k in [0,1,2,3,4,5,6,7,8]:\n",
    "    count = 0\n",
    "    limit = face_emb_labels.count(k)\n",
    "    print(k)\n",
    "    for file in os.listdir(os.path.join(directory_path,str(k))):\n",
    "        if(count==limit):\n",
    "            break\n",
    "        else: \n",
    "            if(count==0):\n",
    "                for file1 in os.listdir(os.path.join(directory_path1,str(k))):\n",
    "                    file_path = os.path.join(directory_path1,str(k))\n",
    "                    file_path = os.path.join(file_path,file1)\n",
    "                    jc_orig = load_image(file_path)\n",
    "                    bb = alignment.getLargestFaceBoundingBox(jc_orig)\n",
    "                    jc_aligned = alignment.align(64, jc_orig, bb, landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n",
    "                    jc_aligned = jc_aligned/255.0\n",
    "            face_npy.append(jc_aligned)\n",
    "            count+=1\n",
    "                                 \n",
    "len(face_npy)==len(voice_embedd_labels)  \n",
    "\n",
    "train_face, test_face, train_voice, test_voice = train_test_split(face_npy, voice_embedd, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sz = len(train_face)\n",
    "num_examples = dataset_sz\n",
    "num_steps = 10000\n",
    "lr_generator = 0.002\n",
    "lr_discriminator = 0.002\n",
    "sound_vector = 128\n",
    "BATCH_SIZE = 64\n",
    "NOISE_SIZE = 128\n",
    "n_batches = int(dataset_sz/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1, f2, f3, f4, f5 = 3, 64, 128, 256, 512\n",
    "s1, s2, s3, s4, s5 = 64, 32, 16, 8, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_completed = 0\n",
    "index_in_epoch = 0\n",
    "num_examples = len(train_face)\n",
    "\n",
    "def next_batch(batch_size):    \n",
    "\n",
    "    global train_face\n",
    "    global train_voice\n",
    "    global index_in_epoch\n",
    "    global epochs_completed\n",
    "\n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "\n",
    "    # when all trainig data have been already used, it is reorder randomly    \n",
    "    if index_in_epoch > num_examples:\n",
    "        # finished epoch\n",
    "        epochs_completed += 1\n",
    "        # shuffle the data\n",
    "        perm = np.arange(num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        train_face = [train_face[i] for i in perm]\n",
    "        train_voice = [train_voice[i] for i in perm]\n",
    "        # start next epoch\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "        assert batch_size <= num_examples\n",
    "    end = index_in_epoch\n",
    "    return train_face[start:end], train_voice[start:end]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize4gan(im):\n",
    "    im += 1.0 # in [0, 2]\n",
    "    im *= 127.0 # in [0, 255]\n",
    "    return im.astype(np.uint8)\n",
    "\n",
    "def lrelu(x,alpha=0.2):\n",
    "    return tf.maximum(x,alpha*x)\t\n",
    "\n",
    "def conv2d(x, features, kernel=[4,4], strides=[1,2,2,1], name=\"conv_layer\"):\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        weights = weight(shape=kernel + features, name=\"weights\")\n",
    "        biases = bias(shape=[features[-1]], name=\"bias\")\n",
    "        output = tf.nn.conv2d(x, weights, strides=strides, padding='SAME') \n",
    "        output = tf.nn.bias_add(output, biases)\n",
    "        return output\n",
    "\n",
    "def deconv2d(x, features, output_shape, kernel=[4,4], strides=[1,2,2,1], name=\"deconv_layer\"):\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        weights = weight(shape=kernel + features, name=\"weights\")\n",
    "        biases = bias(shape=[features[0]], name=\"bias\")\n",
    "        output = tf.nn.conv2d_transpose(x, weights, output_shape=output_shape, strides=strides, padding='SAME') \n",
    "        return tf.reshape(tf.nn.bias_add(output, biases), output.get_shape())\n",
    "\n",
    "def bias(shape, name):\n",
    "    return tf.get_variable(name, shape,initializer=tf.constant_initializer(0.00000))\n",
    "\n",
    "def weight(shape, name):\n",
    "    return tf.get_variable(name, shape,initializer=tf.glorot_uniform_initializer())\t\n",
    "\n",
    "def dense(x, shape, name):\n",
    "    with tf.variable_scope(name):\n",
    "        weights = weight(shape, name=\"weights\")\n",
    "        biases = bias([shape[-1]], name=\"bias\")\n",
    "        return tf.matmul(x,weights) + biases\n",
    "\n",
    "def batch_norm(inputs, decay=0.9, epsilon=0.00001, scale=True, isTrain=True, name=\"batch_norm\"):\n",
    "    return tf.contrib.layers.batch_norm(inputs, decay=decay, scale=scale, epsilon=epsilon, updates_collections=None, is_training=isTrain, scope=name)\t\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(images, voices, batch_size, reuse):\n",
    "    with tf.variable_scope(\"discriminator\") as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "        output = conv2d(images, features=[f1, f2], name=\"d_conv_layer_1\")\n",
    "        output = lrelu(output)\n",
    "\n",
    "        output = conv2d(output, features=[f2, f3], name=\"d_conv_layer_2\")\n",
    "        output = batch_norm(output, isTrain=True, name=\"d_batch_norm_2\")\n",
    "        output = lrelu(output)\n",
    "\n",
    "        output = conv2d(output, features=[f3, f4], name=\"d_conv_layer_3\")\n",
    "        output = batch_norm(output, isTrain=True, name=\"d_batch_norm_3\")\n",
    "        output = lrelu(output)\n",
    "\n",
    "        output = conv2d(output, features=[f4, f5], name=\"d_conv_layer_4\")\n",
    "        output = batch_norm(output, isTrain=True, name=\"d_batch_norm_4\")\n",
    "        output = lrelu(output)\n",
    "\n",
    "        voice_embeddings = dense(voices, shape=[128,128], name=\"d_dense_voice\")\n",
    "        voice_embeddings = lrelu(batch_norm(voice_embeddings, isTrain=True, name='d_batch_norm_5'))\n",
    "        voice_embeddings = tf.expand_dims(voice_embeddings,1)\n",
    "        voice_embeddings = tf.expand_dims(voice_embeddings,2)\n",
    "        tiled_voice_embeddings = tf.tile(voice_embeddings, [1,4,4,1], name='d_tiled_voice_embeddings')\n",
    "\n",
    "        output = tf.concat([output, tiled_voice_embeddings], 3, name='d_concat')\n",
    "        output = conv2d(output, features=[f5+128, f5], strides=[1,1,1,1], name=\"d_conv_layer_5\")\n",
    "        output = batch_norm(output, isTrain=True, name=\"d_batch_norm_8\")\n",
    "        output = lrelu(output)\n",
    "\n",
    "        output = tf.reshape(output, [batch_size, -1])\n",
    "\n",
    "        output = dense(output, [s5*s5*f5, 1], name=\"d_dense_2\")\n",
    "        return output, tf.nn.sigmoid(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampler(voice_embeddings, batch_size):\n",
    "    with tf.variable_scope(\"generator\") as scope:\n",
    "        scope.reuse_variables()\n",
    "        \n",
    "        voice_embeddings = dense(voice_embeddings, shape=[128,128], name=\"g_dense_voice\")\n",
    "\n",
    "        output = dense(voice_embeddings, shape=[128, s5*s5*f5], name=\"g_dense_1\")\n",
    "        output = batch_norm(output, isTrain=False, name=\"g_batch_norm_0\")\n",
    "        output = tf.nn.relu(output)\n",
    "        output = tf.reshape(output, [-1, s5, s5, f5])\n",
    "\n",
    "        # 4x4x512\n",
    "        output = deconv2d(output, features=[f4, f5], output_shape=[batch_size,s4,s4,f4], name=\"g_deconv_layer_1\")\n",
    "        output = batch_norm(output, isTrain=False, name=\"g_batch_norm_1\")\n",
    "        output = tf.nn.relu(output)\n",
    "\n",
    "        # 8x8x256\n",
    "        output = deconv2d(output, features=[f3, f4], output_shape=[batch_size,s3,s3,f3], name=\"g_deconv_layer_2\")\n",
    "        output = batch_norm(output, isTrain=False, name=\"g_batch_norm_2\")\n",
    "        output = tf.nn.relu(output)\n",
    "\n",
    "        # 16x16x128\n",
    "        output = deconv2d(output, features=[f2, f3], output_shape=[batch_size,s2,s2,f2], name=\"g_deconv_layer_3\")\n",
    "        output = batch_norm(output, isTrain=False, name=\"g_batch_norm_3\")\n",
    "        output = tf.nn.relu(output)\n",
    "\n",
    "        # 32x32x64\n",
    "        output = deconv2d(output, features=[f1, f2], output_shape=[batch_size,s1,s1,f1], name=\"g_deconv_layer_4\")\n",
    "        output = tf.nn.tanh(output)\n",
    "        \n",
    "        # 64x64x3\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(voice_embeddings, batch_size):\n",
    "    with tf.variable_scope(\"generator\") as scope:\n",
    "        \n",
    "        voice_embeddings = dense(voice_embeddings, shape=[128,128], name=\"g_dense_voice\")\n",
    "\n",
    "        \n",
    "        output = dense(voice_embeddings, shape=[128, s5*s5*f5], name=\"g_dense_1\")\n",
    "        output = batch_norm(output, isTrain=False, name=\"g_batch_norm_0\")\n",
    "        output = tf.nn.relu(output)\n",
    "        output = tf.reshape(output, [-1, s5, s5, f5])\n",
    "\n",
    "        # 4x4x512\n",
    "        output = deconv2d(output, features=[f4, f5], output_shape=[batch_size,s4,s4,f4], name=\"g_deconv_layer_1\")\n",
    "        output = batch_norm(output, isTrain=True, name=\"g_batch_norm_1\")\n",
    "        output = tf.nn.relu(output)\n",
    "        \n",
    "        # 8x8x256\n",
    "        output = deconv2d(output, features=[f3, f4], output_shape=[batch_size,s3,s3,f3], name=\"g_deconv_layer_2\")\n",
    "        output = batch_norm(output, isTrain=True, name=\"g_batch_norm_2\")\n",
    "        output = tf.nn.relu(output)\n",
    "\n",
    "        # 16x16x128\n",
    "        output = deconv2d(output, features=[f2, f3], output_shape=[batch_size,s2,s2,f2], name=\"g_deconv_layer_3\")\n",
    "        output = batch_norm(output, isTrain=True, name=\"g_batch_norm_3\")\n",
    "        output = tf.nn.relu(output)\n",
    "\n",
    "        # 32x32x64\n",
    "        output = deconv2d(output, features=[f1, f2], output_shape=[batch_size,s1,s1,f1], name=\"g_deconv_layer_4\")\n",
    "        output = tf.nn.tanh(output)\n",
    "        \n",
    "        # 64x64x3\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "#z = tf.placeholder(tf.float32, shape=[None, NOISE_SIZE], name=\"z\")\n",
    "voice_embeddings = tf.placeholder(tf.float32, shape=[None, 128], name=\"voice_embeddings\")\n",
    "real_images = tf.placeholder(tf.float32, shape=[None, s1, s1, f1], name=\"real_input\")\n",
    "real_images_flat = tf.reshape(real_images, [-1,64*64*3])\n",
    "\n",
    "fake_images = generator(voice_embeddings, batch_size=BATCH_SIZE)\n",
    "fake_images_flat = tf.reshape(fake_images, [-1,64*64*3])\n",
    "\n",
    "real_img_real_label_disc_logits, real_disc_real = discriminator(real_images, voice_embeddings,batch_size=BATCH_SIZE, reuse=False)\n",
    "real_img_fake_label_disc_logits, real_disc_fake = discriminator(real_images, voice_embeddings,batch_size=BATCH_SIZE, reuse=True)\n",
    "sample = sampler(voice_embeddings, batch_size=BATCH_SIZE)\n",
    "fake_disc_logits, fake_disc = discriminator(fake_images,voice_embeddings, batch_size=BATCH_SIZE, reuse=True)\n",
    "\n",
    "generation_loss = tf.reduce_sum(tf.maximum(fake_images_flat, 0) - fake_images_flat * real_images_flat\\\n",
    "                                                 + tf.log(1 + tf.exp(-tf.abs(fake_images_flat))), 1)\n",
    "tf.cast(generation_loss,tf.int32)\n",
    "\n",
    "g_loss_1 = tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_disc_logits, labels=tf.zeros_like(fake_disc)+\n",
    "                                                                tf.random_uniform(minval=0,maxval=0.3,shape=tf.shape(fake_disc)))\n",
    "g_loss  = tf.reduce_mean(g_loss_1 + generation_loss)\n",
    "d_loss_real_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real_img_real_label_disc_logits, labels=tf.zeros_like(real_disc_real)+\n",
    "                                                                          tf.random_uniform(minval=0,maxval=0.3,shape=tf.shape(real_disc_real))))\n",
    "d_loss_real_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real_img_fake_label_disc_logits, labels=tf.ones_like(real_disc_fake)-\n",
    "                                                                          tf.random_uniform(minval=0,maxval=0.3,shape=tf.shape(real_disc_fake))))\n",
    "d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_disc_logits, labels=tf.ones_like(fake_disc)-\n",
    "                                                                     tf.random_uniform(minval=0,maxval=0.3,shape=tf.shape(fake_disc))))\n",
    "\n",
    "d_loss = d_loss_fake + (d_loss_real_real + d_loss_real_fake)/2\n",
    "t_vars = tf.trainable_variables()\n",
    "\n",
    "d_vars = [var for var in t_vars if 'd_' in var.name]\n",
    "g_vars = [var for var in t_vars if 'g_' in var.name]\n",
    "\n",
    "d_optim = tf.train.AdamOptimizer(0.0002, beta1=0.5).minimize(d_loss, var_list=d_vars)\n",
    "g_optim = tf.train.AdamOptimizer(0.0001, beta1=0.5).minimize(g_loss, var_list=g_vars)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(20):\n",
    "        dl, gl = [],[]\n",
    "        for i in range(n_batches):\n",
    "            #batch_noise = np.random.uniform(-1,1,size=(BATCH_SIZE, NOISE_SIZE)).astype(np.float32)\n",
    "            images_embedding, voice_embedding = next_batch(BATCH_SIZE)\n",
    "            _, DLOSS = sess.run([d_optim, d_loss],feed_dict={real_images:images_embedding,voice_embeddings:voice_embedding})\n",
    "            \n",
    "            # Update G network\n",
    "            _, GLOSS = sess.run([g_optim, g_loss],feed_dict={real_images:images_embedding,voice_embeddings:voice_embedding})\n",
    "\n",
    "            # Update G network\n",
    "            _, GLOSS = sess.run([g_optim, g_loss],feed_dict={real_images:images_embedding,voice_embeddings:voice_embedding})\n",
    "\n",
    "            dl.append(DLOSS)\n",
    "            gl.append(GLOSS)\n",
    "        print('discriminator_loss / generator_loss => %.2f / %.2f for step %d'%(np.mean(dl), np.mean(gl), epoch))\n",
    "\n",
    "    j=0\n",
    "    for k in range(0,len(test_voice)-64,64):\n",
    "        voice_embedding = test_voice[k:k+64]\n",
    "        image_embedding = test_face[k:k+64]\n",
    "        fake_image = sess.run(sample, feed_dict={real_images:images_embedding,voice_embeddings:voice_embedding})\n",
    "        fake_image = denormalize4gan(fake_image)\n",
    "        for image in fake_image:\n",
    "            plt.imsave('test_gan_sample_1/'+str(j)+'.png',image)\n",
    "            j+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
