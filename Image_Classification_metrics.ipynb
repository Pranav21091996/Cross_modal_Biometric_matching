{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import os\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "def download_landmarks(dst_file):\n",
    "    url = 'http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2'\n",
    "    decompressor = bz2.BZ2Decompressor()\n",
    "    \n",
    "    with urlopen(url) as src, open(dst_file, 'wb') as dst:\n",
    "        data = src.read(1024)\n",
    "        while len(data) > 0:\n",
    "            dst.write(decompressor.decompress(data))\n",
    "            data = src.read(1024)\n",
    "\n",
    "dst_dir = 'models'\n",
    "dst_file = os.path.join(dst_dir, 'landmarks.dat')\n",
    "\n",
    "if not os.path.exists(dst_file):\n",
    "    os.makedirs(dst_dir)\n",
    "    download_landmarks(dst_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import create_model\n",
    "nn4_small2 = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn4_small2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Layer\n",
    "\n",
    "in_a = Input(shape=(96,96,3))\n",
    "in_p = Input(shape=(96,96,3))\n",
    "in_n = Input(shape=(96,96,3))\n",
    "\n",
    "emb_a = nn4_small2(in_a)\n",
    "emb_p = nn4_small2(in_p)\n",
    "emb_n = nn4_small2(in_n)\n",
    "\n",
    "class TripletLossLayer(Layer):\n",
    "    def __init__(self, alpha, **kwargs):\n",
    "        self.alpha = alpha\n",
    "        super(TripletLossLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def triplet_loss(self, inputs):\n",
    "        a, p, n = inputs\n",
    "        p_dist = K.sum(K.square(a-p), axis=-1)\n",
    "        n_dist = K.sum(K.square(a-n), axis=-1)\n",
    "        return K.sum(K.maximum(p_dist - n_dist + self.alpha, 0), axis=0)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        loss = self.triplet_loss(inputs)\n",
    "        self.add_loss(loss)\n",
    "        return loss\n",
    "\n",
    "# Layer that computes the triplet loss from anchor, positive and negative embedding vectors\n",
    "triplet_loss_layer = TripletLossLayer(alpha=0.2, name='triplet_loss_layer')([emb_a, emb_p, emb_n])\n",
    "\n",
    "# Model that can be trained with anchor, positive negative images\n",
    "nn4_small2_train = Model([in_a, in_p, in_n], triplet_loss_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import triplet_generator\n",
    "\n",
    "# triplet_generator() creates a generator that continuously returns \n",
    "# ([a_batch, p_batch, n_batch], None) tuples where a_batch, p_batch \n",
    "# and n_batch are batches of anchor, positive and negative RGB images \n",
    "# each having a shape of (batch_size, 96, 96, 3).\n",
    "generator = triplet_generator() \n",
    "\n",
    "nn4_small2_train.compile(loss=None, optimizer='adam')\n",
    "nn4_small2_train.fit_generator(generator, epochs=10, steps_per_epoch=100)\n",
    "\n",
    "# Please note that the current implementation of the generator only generates \n",
    "# random image data. The main goal of this code snippet is to demonstrate \n",
    "# the general setup for model training. In the following, we will anyway \n",
    "# use a pre-trained model so we don't need a generator here that operates \n",
    "# on real training data. I'll maybe provide a fully functional generator\n",
    "# later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn4_small2_pretrained = create_model()\n",
    "nn4_small2_pretrained.load_weights('weights/nn4.small2.v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "class IdentityMetadata():\n",
    "    def __init__(self, base, file):\n",
    "        # dataset base directory\n",
    "        self.base = base\n",
    "        # image file name\n",
    "        self.file = file\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.image_path()\n",
    "\n",
    "    def image_path(self):\n",
    "        return os.path.join(self.base, self.file) \n",
    "    \n",
    "def load_metadata(path):\n",
    "    metadata = []\n",
    "    for i in os.listdir(path):\n",
    "        ext = os.path.splitext(i)[1]\n",
    "        metadata.append(IdentityMetadata(path, i))\n",
    "    return np.array(metadata)\n",
    "\n",
    "metadata_test_faces = load_metadata('test_faces')\n",
    "metadata_test_gan_sample = load_metadata('test_gan_sample')\n",
    "metadata_test_gan_sample_1 = load_metadata('test_gan_sample_1')\n",
    "metadata_test_vae_sample = load_metadata('test_vae_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from align import AlignDlib\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def load_image(path):\n",
    "    img = cv2.imread(path, 1)\n",
    "    # OpenCV loads images with color channels\n",
    "    # in BGR order. So we need to reverse them\n",
    "    return img[...,::-1]\n",
    "\n",
    "# Initialize the OpenFace face alignment utility\n",
    "alignment = AlignDlib('models/landmarks.dat')\n",
    "\n",
    "# Load an image of Jacques Chirac\n",
    "jc_orig = load_image(metadata_test_faces[100].image_path())\n",
    "\n",
    "# Detect face and return bounding box\n",
    "bb = alignment.getLargestFaceBoundingBox(jc_orig)\n",
    "\n",
    "# Transform image using specified face landmark indices and crop image to 96x96\n",
    "jc_aligned = alignment.align(96, jc_orig, bb, landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n",
    "\n",
    "# Show original image\n",
    "plt.subplot(131)\n",
    "plt.imshow(jc_orig)\n",
    "\n",
    "# Show original image with bounding box\n",
    "plt.subplot(132)\n",
    "plt.imshow(jc_orig)\n",
    "plt.gca().add_patch(patches.Rectangle((bb.left(), bb.top()), bb.width(), bb.height(), fill=False, color='red'))\n",
    "\n",
    "# Show aligned image\n",
    "plt.subplot(133)\n",
    "plt.imshow(jc_aligned);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_image(img):\n",
    "    return alignment.align(96, img, alignment.getLargestFaceBoundingBox(img), \n",
    "                           landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_test_faces = np.zeros((metadata_test_faces.shape[0], 128))\n",
    "embedded_test_gan_sample = np.zeros((metadata_test_gan_sample.shape[0], 128))\n",
    "embedded_test_gan_sample_1 = np.zeros((metadata_test_gan_sample_1.shape[0], 128))\n",
    "embedded_test_vae_sample = np.zeros((metadata_test_vae_sample.shape[0], 128))\n",
    "corrupt = []\n",
    "gan_path = 'test_gan_sample'\n",
    "gan_1_path = 'test_gan_sample_1'\n",
    "vae_path = 'test_vae_sample'\n",
    "test_path = 'test_faces'\n",
    "for i, m in enumerate(metadata_test_vae_sample):\n",
    "    m1=str(m)\n",
    "    file = m1.split('/')\n",
    "    test_img = load_image(os.path.join(test_path,file[1]))\n",
    "    gan_img = load_image(os.path.join(gan_path,file[1]))\n",
    "    gan_1_img = load_image(os.path.join(gan_1_path,file[1]))\n",
    "    vae_img = load_image(os.path.join(vae_path,file[1]))\n",
    "    test_img = align_image(test_img)\n",
    "    gan_img = align_image(gan_img)\n",
    "    gan_1_img = align_image(gan_1_img)\n",
    "    vae_img = align_image(vae_img)\n",
    "    if test_img is not None and gan_img is not None and gan_1_img is not None and vae_img is not None:\n",
    "        # scale RGB values to interval [0,1]\n",
    "        test_img = (test_img / 255.).astype(np.float32)\n",
    "        gan_img = (gan_img / 255.).astype(np.float32)\n",
    "        gan_1_img = (gan_1_img / 255.).astype(np.float32)\n",
    "        vae_img = (vae_img / 255.).astype(np.float32)\n",
    "    # obtain embedding vector for image\n",
    "    try:\n",
    "        embedded_test_faces[i] = nn4_small2_pretrained.predict(np.expand_dims(test_img, axis=0))[0]\n",
    "        embedded_test_gan_sample[i] = nn4_small2_pretrained.predict(np.expand_dims(gan_img, axis=0))[0]\n",
    "        embedded_test_gan_sample_1[i] = nn4_small2_pretrained.predict(np.expand_dims(gan_1_img, axis=0))[0]\n",
    "        embedded_test_vae_sample[i] = nn4_small2_pretrained.predict(np.expand_dims(vae_img, axis=0))[0]\n",
    "    except Exception as e:\n",
    "        corrupt.append(i)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_test_faces = np.delete(embedded_test_faces,corrupt,0)\n",
    "embedded_test_gan_sample = np.delete(embedded_test_gan_sample,corrupt,0)\n",
    "embedded_test_gan_sample_1 = np.delete(embedded_test_gan_sample_1,corrupt,0)\n",
    "embedded_test_vae_sample = np.delete(embedded_test_vae_sample,corrupt,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_test_faces = np.delete(metadata_test_faces,corrupt,0)\n",
    "metadata_test_gan_sample = np.delete(metadata_test_gan_sample,corrupt,0)\n",
    "metadata_test_gan_sample_1 = np.delete(metadata_test_gan_sample_1,corrupt,0)\n",
    "metadata_test_vae_sample = np.delete(metadata_test_vae_sample,corrupt,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  distance (emb1, emb2):\n",
    "    return np.sum(np.square(emb1 - emb2))\n",
    "\n",
    "def show_pair(idx1, idx2):\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.suptitle(f'Distance = {distance(embedded_test_faces[idx1], embedded_test_vae_sample[idx2]):.2f}')\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(load_image(metadata_test_faces[idx1].image_path()))\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(load_image(metadata_test_vae_sample[idx2].image_path()));    \n",
    "\n",
    "show_pair(9, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_distances = []\n",
    "gan_1_distances = []\n",
    "vae_distances = []\n",
    "\n",
    "num = 400\n",
    "\n",
    "for i in range(num):\n",
    "    gan_distances.append(distance(embedded_test_faces[i], embedded_test_gan_sample[i]))\n",
    "    gan_1_distances.append(distance(embedded_test_faces[i], embedded_test_gan_sample_1[i]))\n",
    "    vae_distances.append(distance(embedded_test_faces[i], embedded_test_vae_sample[i]))                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_gan=0\n",
    "correct_gan_1=0\n",
    "correct_vae=0\n",
    "\n",
    "for i in range(400):\n",
    "    if(gan_distances[i]<=0.77):\n",
    "        correct_gan+=1\n",
    "    if(gan_1_distances[i]<=0.77):\n",
    "        correct_gan_1+=1\n",
    "    if(vae_distances[i]<=0.77):\n",
    "        correct_vae+=1    \n",
    "        \n",
    "print(\"Accuracy for CGAN\",(correct_gan/400)*100,\"%\")\n",
    "print(\"Accuracy for RCGAN\",(correct_gan_1/400)*100,\"%\")\n",
    "print(\"Accuracy for VAE\",(correct_vae/400)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
