{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from align import AlignDlib\n",
    "%matplotlib inline\n",
    "\n",
    "alignment = AlignDlib('drive/models/landmarks.dat')\n",
    "def load_image(path):\n",
    "    img = cv2.imread(path, 1)\n",
    "    # OpenCV loads images with color channels\n",
    "    # in BGR order. So we need to reverse them\n",
    "    return img[...,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_embeddings = list(np.load('embedded_face.npy') )\n",
    "face_emb_labels = list(np.load('embedded_face_labels.npy'))\n",
    "voice_embeddings= list(np.load('voice_embeddings_input.npy'))\n",
    "voice_emb_labels= list(np.load('voice_embeddings_labels.npy'))\n",
    "\n",
    "voice_embedd = []\n",
    "voice_embedd_labels = []\n",
    "face_npy = []\n",
    "\n",
    "for k in [0,1,2,3,4,5,6,7,8]:\n",
    "    count = 0\n",
    "    limit = face_emb_labels.count(k)\n",
    "    for i in range(len(voice_emb_labels)):\n",
    "        if(count==limit):\n",
    "            break\n",
    "        else:    \n",
    "            if(voice_emb_labels[i]==k):\n",
    "                voice_embedd.append(voice_embeddings[i])\n",
    "                voice_embedd_labels.append(voice_emb_labels[i])\n",
    "                count+=1\n",
    "            \n",
    "directory_path = 'Dataset'\n",
    "directory_path1 = 'Dataset1'\n",
    "face_npy = []\n",
    "for k in [0,1,2,3,4,5,6,7,8]:\n",
    "    count = 0\n",
    "    limit = face_emb_labels.count(k)\n",
    "    print(k)\n",
    "    for file in os.listdir(os.path.join(directory_path,str(k))):\n",
    "        if(count==limit):\n",
    "            break\n",
    "        else: \n",
    "            if(count==0):\n",
    "                for file1 in os.listdir(os.path.join(directory_path1,str(k))):\n",
    "                    file_path = os.path.join(directory_path1,str(k))\n",
    "                    file_path = os.path.join(file_path,file1)\n",
    "                    jc_orig = load_image(file_path)\n",
    "                    bb = alignment.getLargestFaceBoundingBox(jc_orig)\n",
    "                    jc_aligned = alignment.align(64, jc_orig, bb, landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n",
    "                    jc_aligned = jc_aligned/255.0\n",
    "            face_npy.append(jc_aligned)\n",
    "            count+=1\n",
    "                                 \n",
    "len(face_npy)==len(voice_embedd_labels)  \n",
    "\n",
    "train_face, test_face, train_voice, test_voice = train_test_split(face_npy, voice_embedd, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_completed = 0\n",
    "index_in_epoch = 0\n",
    "num_examples = len(train_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(batch_size):    \n",
    "\n",
    "    global train_face\n",
    "    global train_voice\n",
    "    global index_in_epoch\n",
    "    global epochs_completed\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    # when all trainig data have been already used, it is reorder randomly    \n",
    "    if index_in_epoch > num_examples:\n",
    "        # finished epoch\n",
    "        epochs_completed += 1\n",
    "        # shuffle the data\n",
    "        perm = np.arange(num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        train_face = [train_face[i] for i in perm]\n",
    "        train_voice = [train_voice[i] for i in perm]\n",
    "        # start next epoch\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "        assert batch_size <= num_examples\n",
    "    end = index_in_epoch\n",
    "    return train_face[start:end], train_voice[start:end]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, in_channel, output_channels, name, reuse=False):\n",
    "    with tf.variable_scope(name, reuse=resue):\n",
    "        w = tf.Variable(tf.truncated_normal([5,5,in_channel,output_channels],stddev=0.1), name = 'w')\n",
    "        b = tf.Variable(tf.zeros(output_channels), name = 'b')\n",
    "    \n",
    "        conv = tf.nn.conv2d(x,w,[1,2,2,1], padding = 'SAME') + b\n",
    "    return conv\n",
    "  \n",
    "def deconv2d(x, output_shape, name, reuse=False):\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        w = tf.Variable(tf.truncated_normal([5,5,output_shape[-1],int(x.get_shape()[-1])],stddev=0.1), name = 'w')\n",
    "        b = tf.Variable(tf.zeros(output_shape[-1]), name = 'b')\n",
    "    \n",
    "        deconv = tf.nn.conv2d_transpose(x,w,output_shape = output_shape, strides = [1,2,2,1])+b\n",
    "    return deconv\n",
    "  \n",
    "def dense(x,input_dim,output_dim,name, reuse = False):\n",
    "    with tf.variable_scope(name,reuse=reuse):\n",
    "        w = tf.Variable(tf.truncated_normal([input_dim,output_dim],stddev=0.1),name = 'w')\n",
    "        b = tf.Variable(tf.zeros(output_dim), name = 'b')\n",
    "    \n",
    "    return tf.matmul(x,w)+b\n",
    "  \n",
    "def encoder(input_embedding):\n",
    "  \n",
    "    h1 = tf.nn.relu(dense(input_embedding,128,128,'dense_1'))\n",
    "    \n",
    "    h2 = tf.nn.relu(dense(h1,128,128,'dense_2'))\n",
    "  \n",
    "    z_mean = dense(h2, 128, 128, 'z_mean_dense')\n",
    "    z_logstd = dense(h2,128,128,'z_stddev_dense')\n",
    "  \n",
    "  return z_mean,z_logstd\n",
    "\n",
    "def decoder(z,reuse=False):\n",
    "  \n",
    "    z_fc = dense(z,128,16*16*32,'z_fc_dense',reuse)\n",
    "  \n",
    "    z_matrix = tf.nn.relu(tf.reshape(z_fc,[-1,16,16,32]))\n",
    "  \n",
    "    h1 = tf.nn.relu(deconv2d(z_matrix,[64,32,32,16],'deconv_1',reuse))\n",
    "    h2 = deconv2d(h1, [64,64,64,3], 'deconv2', reuse)\n",
    "  \n",
    "  return tf.identity(h2)\n",
    "\n",
    "def training_step(sess,input_faces,input_clips):\n",
    "    sess.run(optimizer, feed_dict = {input_images:input_faces,input_voices:input_clips})\n",
    "\n",
    "def loss_step(sess,input_faces,input_clips):\n",
    "    return sess.run(loss, feed_dict = {input_images:input_faces,input_voices:input_clips})\n",
    "\n",
    "def generation_step(sess, z_samples):\n",
    "    return sess.run(generator, feed_dict = {self.z_samples:z_samples})\n",
    "\n",
    "def recognition_step(sess, input_faces, input_clips):\n",
    "    eturn sess.run(generated_images_sigmoid, feed_dict = {input_images:input_faces,input_voices:input_clips})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "latent_dim = 128\n",
    "batch_size = 64\n",
    "\n",
    "# placeholder for input images. Input images are RGB 64x64\n",
    "input_images = tf.placeholder(tf.float32, shape=[None,64,64,3])\n",
    "input_voices = tf.placeholder(tf.float32, shape=[None,128])\n",
    "\n",
    "input_images_flat = tf.reshape(input_images, [-1,64*64*3])\n",
    "\n",
    "# placeholder for z_samples. We are using this placeholder when we are generating new images\n",
    "z_samples = tf.placeholder(tf.float32, shape=[None,latent_dim])\n",
    "\n",
    "# encoder\n",
    "z_mean,z_logstd = encoder(input_voices)\n",
    "\n",
    "#decoder input\n",
    "samples = tf.random_normal([batch_size,latent_dim], 0, 1, dtype = tf.float32 )\n",
    "z = z_mean + (tf.exp(.5*z_logstd)*samples)\n",
    "\n",
    "#decoder\n",
    "generated_images = decoder(z)\n",
    "generated_images_sigmoid = tf.sigmoid(generated_images)\n",
    "generated_images_flat = tf.reshape(generated_images, [-1,64*64*3])\n",
    "\n",
    "#loss Calculation\n",
    "\n",
    "generation_loss = tf.reduce_sum(tf.maximum(generated_images_flat, 0) - generated_images_flat * input_images_flat\\\n",
    "                                + tf.log(1 + tf.exp(-tf.abs(generated_images_flat))), 1)\n",
    "\n",
    "latent_loss = 0.5 * tf.reduce_sum(tf.square(z_mean) + tf.exp(2*z_logstd) - 2*z_logstd - 1, 1)\n",
    "\n",
    "loss = tf.reduce_mean(latent_loss + generation_loss)\n",
    " \n",
    "#optimizer\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5000\n",
    "interval = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())  \n",
    "    while(epochs_completed < num_epochs):\n",
    "        current_epoch = epochs_completed\n",
    "        step = 0    \n",
    "        while(epochs_completed < current_epoch+1):\n",
    "            input_faces,input_clips = next_batch(batch_size)\n",
    "            training_step(sess,input_faces,input_clips)\n",
    "            step+=1\n",
    "            if(step%interval==0):\n",
    "                print('loss : {} '.format(loss_step(sess,input_faces,input_clips)))    \n",
    "        print('Epochs {} completed'.format(current_epoch))  \n",
    "    j=0\n",
    "    for k in range(0,len(test_voice)-64,64):\n",
    "        output_images = recognition_step(sess,test_face[k:k+64], test_voice[k:k+64])\n",
    "        output_images = output_images * 255\n",
    "        output_images = output_images.astype(np.uint8)\n",
    "        for image in output_images:\n",
    "            plt.imsave('test_vae_sample/'+str(j)+'.png',image)\n",
    "            j+=1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "\n",
    "for image in output_images:\n",
    "    plt.imsave('drivea/sample/'+str(i)+'.jpg',image)\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
